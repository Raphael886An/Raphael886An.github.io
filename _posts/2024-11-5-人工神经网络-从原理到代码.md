感知机只能处理二分类问题，而且必须是线性可分的问题
## 人工神经网络
人工神经网络（Artificial Neural Network），简称ANN，其实感知机模型就可以看做一个简单的单层神经网络
这里的ANN，特指多层感知机

这是感知机简化过的工作流程
![2024-11-5-1](/img/2024-11-5/2024-11-5-1.jpg)
这张图中，我们可以把输入称之为【输入层】，输出称之为【输出层】，对于这样只包含一个输入层的网络结构就可以称之为单层神经网络结构

单个感知机组成了单层神经网络，那么把一个感知机的输出作为另一个感知机的输入，就组成了多层感知机，也就是一个多层神经网络

其中，我们将输入和输出层之间的称之为隐含层，如下图就是，包含一个隐含层的神经网络结构
![2024-11-5-2](/img/2024-11-5/2024-11-5-2.jpg)
一个神经网络结构在计算层数的时候，一般只计算输入层和隐含层的数量，如上图就是一个两层的神经网络结构
## 激活函数
激活函数【Activation function】的作用，就是在网络结构中引入非线性的因素，这样就可以解决线性模型无法完成的分类任务
### sigmoid函数
sigmoid函数公式如下

$$
sigmoid(x)= \frac{1}{1+e^{-x}}
$$

他的函数图像呈S型，函数值位于（0，1）之间：
![2024-11-5-3](/img/2024-11-5/2024-11-5-3.jpg)

### Tanh函数
Tanh函数与sigmoid函数很相似，都呈S型，只不过Tanh函数值介于（-1，1）之间，公式如下：

$$
tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}
$$

![2024-11-5-4](/img/2024-11-5/2024-11-5-4.jpg)
### ReLU函数
ReLU函数的全称叫做Rectified Linear Unit，翻译过来就是修正线性单元，函数公式如下：

$$
ReLU(x)=max(0,x)
$$

ReLU有很多优点，比如收敛速度较快的同时还不容易出现梯度消失，图像如下：
![2024-11-5-5](/img/2024-11-5/2024-11-5-5.jpg)
## 反向传播算法
下图是一个经典的三层神经网络结构，其中包含有两个输入$x_1$和$x_2$以及1个输出$y$
![2024-11-5-6](/img/2024-11-5/2024-11-5-6.jpg)
图中每个紫色单元单标一个独立的神经元，其中$e$代表激活信号，所以$y=f(e)$就是被激活函数处理之后的非线性输出，也就是整个神经元的输出
![2024-11-5-7](/img/2024-11-5/2024-11-5-7.jpg)
下面开始训练神经网络，这里只给出第一个例子，首先计算第一个隐含层中第一个神经元$y_1=f_1(e)$对应的值
![2024-11-5-8](/img/2024-11-5/2024-11-5-8.jpg)
这个从$x$到$y$的过程，就是前向传播的过程

那么，什么是反向传播呢

当我们得到输出结果$y$时，可以与期望输出$z$对比得到误差$\delta$
![2024-11-5-9](/img/2024-11-5/2024-11-5-9.jpg)
然后，我们将误差$\delta$沿着神经元回路反向传递到前一个隐含层，

注意每个神经元对应的误差要乘以权重，以此类推，我们得到所有的误差
![2024-11-5-10](/img/2024-11-5/2024-11-5-10.jpg)
当我们得到所有的反向传递过来的误差时，我们就可以利用这个误差对输入层到第一个隐含层之间的权重$w$进行更新
![2024-11-5-11](/img/2024-11-5/2024-11-5-11.jpg)
图中的$\eta$表示学习速率，以此类推，再将所有的神经元单元重新更新一次。这就完成了一个迭代过程

更新完权重之后，又开始下一轮的前向传播得到输出，再开始反向传播误差更新权重，依次迭代下去

所以反向传播其实代表的就是反向传播误差
## python实现人工神经网络
这里只构建包含一个隐含层的人工神经网络结构。其中，输入层为2个神经元，隐含层为3个神经元，并通过输出层实现2分类问题的求解。该神经网络的结构如下：
![2024-11-5-12](/img/2024-11-5/2024-11-5-12.jpg)
在这个例子中，我们使用的激活函数为$sigmoid$函数:

$$
sigmodi(x)=\frac{1}{1+e^{-x}}
$$

所以，其导数就是

$$
\Delta sigmoid(x)=sigmoid(x)(1-sigmoid(x))
$$

python代码就是

```
def sigmoid(x):
    # sigmoid 函数
    return 1 / (1 + np.exp(-x))


def sigmoid_derivative(x):
    # sigmoid 函数求导
    return sigmoid(x) * (1 - sigmoid(x))
```

### 前向传播
在前向传播中，每个神经元的计算流程为：线性变换$\rightarrow$激活函数$\rightarrow$输出值

同时，我们约定

$Z$表示隐含层输出，$Y$表示输出层最终输出

$W_ij$表示第$i$层的第$j$个权重

于是，上图中的前向传播计算就是

$$
X=\begin{bmatrix}x_1 & x_2\end{bmatrix}
$$

$$
W_1=\begin{bmatrix}
W_{11} & W_{12} & W_{13}\\ W_{14} & W_{15} & W_{16}
\end{bmatrix}
$$

$$
W_2=\begin{bmatrix}
W_{21} \\ W_{22} \\ W_{23} \end{bmatrix}
$$

接下来，计算隐含层的神经元输出$Z$(线性变化$\rightarrow$激活函数)
同样，为了使得计算过程足够清晰，这里将截距项表示为0

$$
Z=sigmoid(X·W_1)
$$

最后，计算输出层$Y$(线性变化$\rightarrow$激活函数)

$$
Y=sigmoid(Z·W_2)
$$

下面是代码实现部分
```
# 示例样本
X = np.array([[1, 1]])
y = np.array([[1]])

X, y
```

```
#运行输出
(array([[1, 1]]), array([[1]]))
```

然后 随机初始化隐含层权重
```
W1 = np.random.rand(2, 3)
W2 = np.random.rand(3, 1)

W1, W2
```

得到输出
```
(array([[0.71350155, 0.17327771, 0.16294774],
        [0.43576633, 0.59924959, 0.43553143]]),
 array([[0.49062887],
        [0.43493371],
        [0.18689141]]))
```

然后基于上面前向传播的公式，完成
```
input_layer = X  # 输入层
hidden_layer = sigmoid(np.dot(input_layer, W1))  # 隐含层，公式 20
output_layer = sigmoid(np.dot(hidden_layer, W2))  # 输出层，公式 22

output_layer
```

得到输出

```
array([[0.68798137]])
```

### 反向传播
我们还是使用梯度下降的方法来优化神经网络的参数

那么，首先需要定义损失函数，然后计算损失函数关于神经网络中各层的权重的偏导数，也就是梯度

此时，设神经网络的输出值为$Y$,真实值为$y$。然后，定义平方损失函数如下：

$$
Loss(y,Y)=Σ{(y-Y)^2}
$$

接下来，求解梯度 $(\frac{\partial Loss(y, Y)}{\partial W_2})$，需要使用链式求导法则：

$$
\frac{\partial Loss(y, Y)}{\partial W_2} = \frac{\partial Loss(y, Y)}{\partial Y} \cdot \frac{\partial Y}{\partial W_2}
$$

$$
\frac{\partial Loss(y, Y)}{\partial W_2} = 2(Y - y) * \Delta sigmoid(Z \cdot W_2) \cdot Z
$$

```
d_W2 = np.dot(
    hidden_layer.T,
    (2 * (output_layer - y) * sigmoid_derivative(np.dot(hidden_layer, W2))),
)
```

同理，梯度 $(frac{\partial Loss(y, Y)}{\partial W_1})$ 得：

$$
\frac{\partial Loss(y, Y)}{\partial W_1} = \frac{\partial Loss(y, Y)}{\partial Y} \cdot \frac{\partial Y}{\partial Z} \cdot \frac{\partial Z}{\partial W_1}
$$

$$
\frac{\partial Loss(y, Y)}{\partial W_1} = 2(Y - y) * \Delta sigmoid(Z \cdot W_2) \cdot W_2 * \Delta sigmoid(X \cdot W_1) \cdot X
$$

```
d_W1 = np.dot(
    input_layer.T,
    (
        np.dot(
            2 * (output_layer - y) * sigmoid_derivative(np.dot(hidden_layer, W2)), W2.T
        )
        * sigmoid_derivative(np.dot(input_layer, W1))
    ),
)

d_W2, d_W1
```
得到输出

```
(array([[-0.10172443],
        [-0.0916361 ],
        [-0.08644402]]),
 array([[-0.01200923, -0.01259169, -0.00573027],
        [-0.01200923, -0.01259169, -0.00573027]]))
```

现在就可以设置学习率，对$W_1W_2$进行一次更新

```
# 梯度下降更新权重, 学习率为 0.05

W1 -= 0.05 * d_W1  # 如果上面是 y - output_layer，则改成 +=
W2 -= 0.05 * d_W2

W2, W1
```

得到输出
```
(array([[0.49571509],
        [0.43951551],
        [0.19121361]]),
 array([[0.71410201, 0.1739073 , 0.16323425],
        [0.43636679, 0.59987918, 0.43581795]]))
```

以上就是单个样本在神经网络的一次前向传播+反向传播，并使用梯度下降来进行一次更新

### 神经网络的完整实现

```
# 示例神经网络完整实现
class NeuralNetwork:
    # 初始化参数
    def __init__(self, X, y, lr):
        self.input_layer = X
        self.W1 = np.random.rand(self.input_layer.shape[1], 3)
        self.W2 = np.random.rand(3, 1)
        self.y = y
        self.lr = lr
        self.output_layer = np.zeros(self.y.shape)

    # 前向传播
    def forward(self):
        self.hidden_layer = sigmoid(np.dot(self.input_layer, self.W1))
        self.output_layer = sigmoid(np.dot(self.hidden_layer, self.W2))

    # 反向传播
    def backward(self):
        d_W2 = np.dot(
            self.hidden_layer.T,
            (
                2
                * (self.output_layer - self.y)
                * sigmoid_derivative(np.dot(self.hidden_layer, self.W2))
            ),
        )

        d_W1 = np.dot(
            self.input_layer.T,
            (
                np.dot(
                    2
                    * (self.output_layer - self.y)
                    * sigmoid_derivative(np.dot(self.hidden_layer, self.W2)),
                    self.W2.T,
                )
                * sigmoid_derivative(np.dot(self.input_layer, self.W1))
            ),
        )

        # 参数更新
        self.W1 -= self.lr * d_W1
        self.W2 -= self.lr * d_W2
```

使用一开始的示例数据集测试，但是要对数据形状进行调整，以满足需要
```
X = df[["X0", "X1"]].values  # 输入值
y = df[["Y"]].values  # 真实 y
```

接下啦，使用神经网络来进行训练，并且迭代100次
```
nn = NeuralNetwork(X, y, lr=0.001)  # 定义模型
loss_list = []  # 存放损失数值变化

for i in range(100):
    nn.forward()  # 前向传播
    nn.backward()  # 反向传播
    loss = np.sum((y - nn.output_layer) ** 2)  # 计算平方损失
    loss_list.append(loss)

print("final loss:", loss)
plt.plot(loss_list)  # 绘制 loss 曲线变化图
```

得到输出
```
final loss: 133.3883521578235
```
![2024-11-5-13](/img/2024-11-5/2024-11-5-13.jpg)
