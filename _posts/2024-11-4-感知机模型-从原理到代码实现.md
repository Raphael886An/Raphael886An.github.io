# 感知机是什么
感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。

在二分类模型中，我们最终的类别是由两个，被称之为正类别和负类别。我们使用以下Sign公式完成分类时

$$
\operatorname{sign}(x)=\left\{\begin{array}{ll}+1,&\text{ if} x\geq 0\\ -1,&\text{ if} x<0\end{array}\right.
$$

Sign函数又被称之为符号函数，它的函数值只有两个。即自变量x≥0时，因变量为1。函数图像如下：
![2024-11-4-1](/img/2024-11-4/2024-11-4-1.jpg)
所以在这个函数中，当 
$$
\operatorname{sign}(f(x))=1
$$
时，就为正分类点

因此，我们不难得出，感知机的分类函数为

$$
\operatorname{f}(x)={sign}{(W*x+b)}
$$

这个函数就是感知机函数
流程图如下
![2024-11-4-2](/img/2024-11-4/2024-11-4-2.jpg)
# 感知机的损失函数
在感知机的学习过程中，我们同样需要确定每一个特征变量对应的参数，二损失函数的极小值往往就意味着参数的最佳值

在感知机的学习过程中，可能会出现误分类的问题
![2024-11-4-3](/img/2024-11-4/2024-11-4-3.jpg)
在这一过程中，我OEM使用误分类点到分割线(或者是误分类面)的距离去定义损失函数
## 点到直线的距离
我们都学过点到直线的距离是

$$
\operatorname{W}*{x+b=0}
$$

我们需要把这一公式推导到n维向量空间中，我们使用投影法
我们将
$$
\operatorname{W}*{x+b=0}
$$视为一个超平面，那么此时就有一个向量位于通过 
$$
x_0 
$$
且方向为
$$
\mathbf{W}
$$
的直线上，这个向量就是
$$
\operatorname{x_0}+{tW}
$$ 

因此我们需要找到t的值，使得
$$
\operatorname{x_0}+{tW}
$$
位于超平面上，也就是
$$
\operatorname{x_0}+{tW}={0}
$$

我们将

$$
\operatorname{x_0}+{tW}
$$

代入

$$
\operatorname{W}*{x+b=0}
$$

即可得到

$$
\operatorname{W}*{（x_0+tW）x+b=0}
$$

$$
t = -\frac{W x_0 + b}{W^2}
$$

将t带回向量
$$
\operatorname{x_0}+{tW}
$$
中
即可得到

$$
x_0 -\frac{W x_0 + b}{W^2}W
$$

此时，距离d是该值的范数

因此

$$
d = \left\| x_0 - \frac{W x_0 + b}{W^2} W \right\|
$$

化简该式子，即可得到

$$
d=\frac{I}{\|M\|}|W*x_0 + b |
$$

其中
$$
\|M\|
$$
表示
$$
L_2
$$
范数，是各向量的平方和再开方
假设所有误分类的点的集和是M，那么所有错误分类的点到分割线（分割面）的距离就是：

$$
-\frac{1}{\|W\|}\sum_{x_{i} \in M} y_{i}\left(W* x_{i} + b\right)
$$

我们使用梯度下降的方法来实现分类的时候

也就是最后的损失函数是

$$
J（W,b）=-\frac{1}{\|W\|}\sum_{x_{i} \in M} y_{i}\left(W* x_{i} + b\right)
$$

不难看出，
$$
J(W,b)
$$
一定是非负的。也就是说，当没有误分类的点时，损失函数的值就是0。错误分类的点越少，损失函数的值就越小。同时，该函数也是一个连续可导函数
## 随机梯度下降法
我们使用SGD公式计算损失函数的极小值时，任选一个分割线

$$
min_W,bJ（W,b）=-\frac{1}{\|W\|}\sum_{x_{i} \in M} y_{i}\left(W* x_{i} + b\right)
$$

计算其偏导数

$$
\frac{\partial J(W, b)}{\partial W} = - \sum_{x_i \in M} y_i x_i
$$

如果
$$
y_i(W*x_i+b)≤0
$$
更新到W和b
那么

$$
W \leftarrow W + \lambda y_i x_i
$$

$$
b \leftarrow b + \lambda y_i
$$

我们使用sklearn来实现随机梯度下降

```
from sklearn.utils import shuffle


def perceptron_sgd(X, Y, alpha, epochs):

    # 感知机随机梯度下降算法实现
    w = np.zeros(len(X[0]))  # 初始化参数为 0
    b = np.zeros(1)

    for t in range(epochs):  # 迭代
        # 每一次迭代循环打乱训练样本
        # X, Y = shuffle(X, Y)
        for i, x in enumerate(X):
            if ((np.dot(X[i], w) + b) * Y[i]) <= 0:  # 判断条件
                w = w + alpha * X[i] * Y[i]  # 更新参数
                b = b + alpha * Y[i]

    return w, b
    ###参数:
    ###X -- 自变量数据矩阵
    ###Y -- 因变量数据矩阵
    ###alpha -- lamda 参数
    ###epochs -- 迭代次数

    ###返回:
    ###w -- 权重系数
    ###b -- 截距项
    ###这里注销了随机打乱样本数据
```
# 代码实现
我们使用这个数据来进行代码实现
```
wget -nc https://cdn.aibydoing.com/aibydoing/files/course-12-data.csv
```
```
import pandas as pd

df = pd.read_csv("course-12-data.csv", header=0)  # 加载数据集
df.head()  # 预览前 5 行数据
```
![2024-11-4-4](/img/2024-11-4/2024-11-4-4.jpg)

我们来看一下数据的分布情况
```
from matplotlib import pyplot as plt

%matplotlib inline

# 绘制数据集
plt.figure(figsize=(10, 6))
plt.scatter(df["X0"], df["X1"], c=df["Y"])
```
![2024-11-4-5](/img/2024-11-4/2024-11-4-5.jpg)
##训练过程
```
import numpy as np

X = df[["X0", "X1"]].values
Y = df["Y"].values

alpha = 0.1
epochs = 150

perceptron_sgd(X, Y, alpha, epochs)
###先将学习率设置为0.1
```
![2024-11-4-6](/img/2024-11-4/2024-11-4-6.jpg)
所以，我们求得的分割线的方程就是

$$
f(x)=4.93*x_1-6.98*x_2-3.3
$$

此时，分类的正确率是
```
L = perceptron_sgd(X, Y, alpha, epochs)
w1 = L[0][0]
w2 = L[0][1]
b = L[1]

z = np.dot(X, np.array([w1, w2]).T) + b
np.sign(z)

from sklearn.metrics import accuracy_score

accuracy_score(Y, np.sign(z))
###直接使用scikit-learn现成的包来计算即可
```
得到
![2024-11-4-7](/img/2024-11-4/2024-11-4-7.jpg)
## 绘制决策边界线
```
# 绘制轮廓线图，不需要掌握
plt.figure(figsize=(10, 6))
plt.scatter(df["X0"], df["X1"], c=df["Y"])

x1_min, x1_max = (
    df["X0"].min(),
    df["X0"].max(),
)
x2_min, x2_max = (
    df["X1"].min(),
    df["X1"].max(),
)

xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))
grid = np.c_[xx1.ravel(), xx2.ravel()]

probs = (np.dot(grid, np.array([L[0][0], L[0][1]]).T) + L[1]).reshape(xx1.shape)
plt.contour(xx1, xx2, probs, [0], linewidths=1, colors="red")
```
得到输出
![2024-11-4-8](/img/2024-11-4/2024-11-4-8.jpg)
图中的红色线就是我们最终的分割线
## 绘制loss曲线
```
def perceptron_loss(X, Y, alpha, epochs):
    # 计算每次迭代后的损失函数值
    w = np.zeros(len(X[0]))  # 初始化参数为 0
    b = np.zeros(1)
    loss_list = []

    for t in range(epochs):  # 迭代
        loss_init = 0
        for i, x in enumerate(X):
            # 每一次迭代循环打乱训练样本
            # X, Y = shuffle(X, Y)
            if ((np.dot(X[i], w) + b) * Y[i]) <= 0:  # 判断条件
                loss_init += (np.dot(X[i], w) + b) * Y[i]
                w = w + alpha * X[i] * Y[i]  # 更新参数
                b = b + alpha * Y[i]
        loss_list.append(loss_init * -1)

    return loss_list
    ###参数:
    ###X -- 自变量数据矩阵
    ###Y -- 因变量数据矩阵
    ###alpha -- lamda 参数
    ###epochs -- 迭代次数

    ###返回:
    ###loss_list -- 每次迭代损失函数值列表
```

```
loss_list = perceptron_loss(X, Y, alpha, epochs)

plt.figure(figsize=(10, 6))
plt.plot([i for i in range(len(loss_list))], loss_list)
plt.xlabel("Learning rate {}, Epochs {}".format(alpha, epochs))
plt.ylabel("Loss function")
```
得到输出
![2024-11-9](/img/2024-11-4/2024-11-4-9.jpg)
这个图好像抽风了一样


我们可以看到使用0.1的学习率迭代150次之后，loss仍然无法达到0

当我们的数据不是线性可分的数据的时候，图像就会呈现出上面的这种震荡现象

但是我们的数据实际上是线性可分的，却呈现出这种震荡的情况

这时一般有两个原因：
1. 学习率太大
2. 迭代次数太少

可以看下面的示意图
![2024-11-10](/img/2024-11-4/2024-11-4-10.jpg)

我们可以减小学习率，增加迭代次数，来再试一次，找到损失函数的极小值点

```
alpha = 0.05  # 减小学习率
epochs = 1000  # 增加迭代次数

loss_list = perceptron_loss(X, Y, alpha, epochs)

# Flatten the arrays in loss_list
flattened_loss = [
    item[0] if isinstance(item, np.ndarray) else item for item in loss_list
]

plt.figure(figsize=(10, 6))
plt.plot(range(len(flattened_loss)), flattened_loss)
plt.xlabel("Iterations")
plt.ylabel("Loss function")
```
输出结果如下：
![2024-11-11](/img/2024-11-4/2024-11-4-11.jpg)
可以看到，到了后半段的时候，损失函数的值已经等于0了，大概在700次左右的时候

损失函数为0的时候，就代表没有误分类的点存在了

此时，我们在计算一次准确率
```
L = perceptron_sgd(X, Y, alpha, epochs)
z = np.dot(X, L[0].T) + L[1]
accuracy_score(Y, np.sign(z))
```
![2024-11-12](/img/2024-11-4/2024-11-4-12.jpg)
和损失函数曲线图得到的结果一致，分类准确率已经是100%，表示没有误分类的数据点